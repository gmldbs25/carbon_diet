{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VeganRecipeClassification DeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDYcBfXvSnBY"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.gridspec as gridspec\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "plt.style.use('seaborn')\n",
        "sns.set_style('whitegrid')\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6SQDn9aWKFt"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ksYhMUQTnhn"
      },
      "source": [
        "# 확인 필요\n",
        "# raw 데이터 임포트\n",
        "recipe_df = pd.read_csv('/content/drive/Shareddrives/2021 데이터청년캠퍼스/데이터베이스/recipe_db.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2XZRGuzTncK"
      },
      "source": [
        "# 학습 준비\n",
        "recipe = recipe_df\n",
        "recipe['Vegetarian_class'].unique()\n",
        "\n",
        "# 'Vegetarian_class'열 값이 결측인 행을 제거\n",
        "recipe_test = recipe.dropna(subset = ['Vegetarian_class'], axis=0)\n",
        "recipe_test['Vegetarian_class'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRzUBbBJVG9o"
      },
      "source": [
        "Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGYRrUSzVItC"
      },
      "source": [
        "# column간 상관관계 확인\n",
        "recipe_corr=recipe.corr()\n",
        "f,ax=plt.subplots(figsize=(10,7))\n",
        "sns.heatmap(recipe_corr, cmap='viridis')\n",
        "plt.title(\"Correlation between features\", \n",
        "          weight='bold', \n",
        "          fontsize=18)\n",
        "plt.xticks(weight='bold')\n",
        "plt.yticks(weight='bold')\n",
        "\n",
        "plt.show() # 대부분의 column은 70%이상이 결측으로, 하얗게 표시됨\n",
        "# From the correlation heatmap we can not see a strong correlation between features."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMhZUnioVwY8"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvmYXL3vVzMB"
      },
      "source": [
        "# 학습 데이터, 필요 columns 추출\n",
        "train = pd.DataFrame()\n",
        "train['menu'] = recipe_test['RCP_NM']\n",
        "train['recipe'] = recipe_test['RCP_PARTS_DTLS']\n",
        "train['Vege Class'] = recipe_test['Vegetarian_class']\n",
        "\n",
        "# 결측치 확인\n",
        "train.isnull().sum()\n",
        "\n",
        "# Vege Class 정수로 매핑\n",
        "train['Vege Class'] = train['Vege Class'].replace(['채소', '달걀', '우유(유제품)', '생선,조개', '가금류', '소,돼지'], [0,1,2,3,4,5])\n",
        "\n",
        "\n",
        "veg = train['Vege Class'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8nCPOgcWMlL"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQbd0hRjWOqI"
      },
      "source": [
        "# 확인 필요\n",
        "# 식품 분류표\n",
        "vegetarian_class = pd.read_excel('/content/drive/Shareddrives/2021 데이터청년캠퍼스/데이터베이스/식품별 1g 당 온실가스 배출량(g)_최종.xlsx')\n",
        "\n",
        "# 식품 분류 \n",
        "food_num=vegetarian_class['ING_SEQ']\n",
        "food_name=vegetarian_class['ING_NM']\n",
        "\n",
        "food_category={100:'소고기',200:'양고기',300:'치즈',400:'젖소',500:'초콜렛',600:'커피',700:'새우',800:'돼지고기',900:'가금류 고기',1000:'양식 해산물',1100:'달걀',1200:'쌀',1300:'두부',1400:'우유,유제품',1500:'토마토',1600:'바닐라빈',1700:'식용유',1800:'올리브유',1900:'카놀라유',2000:'해바라기유',2100:'설탕',2200:'땅콩',2300:'오트밀',2400:'콩류',2500:'밀,호밀',2600:'비트 슈가',2700:'와인',2800:'옥수수',2900:'보리 (맥주)',3000:'베리/포도류',3100:'두유',3200:'카사바',3300:'완두콩',3400:'과일류',3500:'채소류',3600:'배추속채소',3700:'양파/부추속채소',3800:'뿌리채소',3900:'사과,감귤류',4000:'견과류'}\n",
        "food_list=dict(zip(food_num,food_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1e5mgiwYJc9"
      },
      "source": [
        "# 토큰 max_len 확인\n",
        "# recipe 재료를 식품 분류표 기준으로 반환\n",
        "\n",
        "rcp = train['recipe'].values.astype(str).tolist()\n",
        "max_len=1\n",
        "\n",
        "for idx,val in enumerate(rcp):\n",
        "  one=val.split(',')\n",
        "  if len(one)>max_len:\n",
        "    max_len=len(one)\n",
        "  for idx2,val2 in enumerate(one):\n",
        "    for key,value in food_list.items():\n",
        "      if value in val2:\n",
        "\n",
        "        one[idx2]=key//1000\n",
        "      \n",
        "  rcp[idx]=one"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wji1wAPxE-o2"
      },
      "source": [
        "# 입력 인자 input_ids\n",
        "input_ids=[]\n",
        "\n",
        "for idx,val in enumerate(rcp):\n",
        "  input_ids_list=[]\n",
        "  for i in val:\n",
        "    if str(type(i))!=\"<class 'int'>\":\n",
        "      input_ids_list.append(0)\n",
        "    elif str(type(i))==\"<class 'int'>\":\n",
        "      input_ids_list.append(i)\n",
        "  input_ids.append(input_ids_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ttypw15GO8j"
      },
      "source": [
        "# 입력 토큰 0으로 패딩\n",
        "for idx, val in enumerate(input_ids):\n",
        "  while len(val)!=max_len:\n",
        "    if len(val)==max_len:\n",
        "      break\n",
        "    else:\n",
        "      val.append(0)\n",
        "\n",
        "for _ in input_ids:\n",
        "  print(_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBNwliYnGbuH"
      },
      "source": [
        "# 데이터 증폭 : 입력 토큰을 랜덤으로 섞어 한 데이터당 20개의 데이터로 만듦\n",
        "import random\n",
        "\n",
        "rand_list=[]\n",
        "rand_veg=[]\n",
        "\n",
        "for idx,val in enumerate(input_ids):\n",
        "  temp_ids=val.copy()\n",
        "  temp_veg=veg[idx]\n",
        "  for i in range(20): #한 input_ids의 줄 당 20번씩 섞기\n",
        "    random.shuffle(temp_ids)\n",
        "    one=temp_ids[:]\n",
        "    rand_list.append(one)\n",
        "    rand_veg.append(temp_veg)\n",
        "\n",
        "input_ids.extend(rand_list)\n",
        "veg.extend(rand_veg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo-W65V-GxH9"
      },
      "source": [
        "# 입력 토큰 attention_masks\n",
        "\"\"\"\n",
        "attention_masks 만들기\n",
        "input_ids 존재값은 1 없는 값은 0\n",
        "\"\"\"\n",
        "attention_masks=[]\n",
        "for idx,val in enumerate(input_ids):\n",
        "  attention_masks_list=[]\n",
        "  for i in val:\n",
        "    if i!=0:\n",
        "      attention_masks_list.append(1)\n",
        "    else:\n",
        "      attention_masks_list.append(0)\n",
        "  attention_masks.append(attention_masks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9_6jxBqG2ov"
      },
      "source": [
        "# 입력 토큰들을 모델 학습을 위해 tensor 형식으로 변경\n",
        "\n",
        "import torch\n",
        "input_ids = torch.LongTensor(input_ids)\n",
        "attention_masks = torch.LongTensor(attention_masks)\n",
        "labels = torch.tensor(veg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS1WVlI6G_jt"
      },
      "source": [
        "# 훈련 데이터 셋을 9:1 비율로 train과 validation 데이터셋으로 분류\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)  #storing the input ids,masks and labels in dataset\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])  #90-10 train-val split\n",
        "\n",
        "# 데이터셋 random sampling\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset),     #random sampling in training\n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset),    #sequential sampling in validation\n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "# MODEL ; BERT\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 6,   \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSzlxvvVHgNy"
      },
      "source": [
        "# optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8 \n",
        "                )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeBZL56_HgK5"
      },
      "source": [
        "# 정확도 계산 함수\n",
        "import numpy as np\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "# 소요 시간 계산 함수\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFFbPqDFH-aj"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeYSI7XAHy1Q"
      },
      "source": [
        "# 훈련 loop\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "seed_val = 42\n",
        "epochs=4\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "total_t0 = time.time()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        \n",
        "        # Printing the progress after every 40 epochs\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            \n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3w13JgoH8F6"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUhByF_uHyxr"
      },
      "source": [
        "# 예측 데이터셋\n",
        "recipe_vali = recipe[recipe['Vegetarian_class'].isnull()]\n",
        "vege_vali = recipe_vali['Vegetarian_class']\n",
        "\n",
        "new_recipe = pd.read_csv('/content/drive/Shareddrives/2021 데이터청년캠퍼스/데이터베이스/Add_recipe_db.csv')\n",
        "new_recipe['Vegetarian_class'] = np.nan\n",
        "recipe_vali2 = pd.DataFrame()\n",
        "recipe_vali2 = pd.concat([recipe_vali, new_recipe])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMCZplBpHyuY"
      },
      "source": [
        "# pre processing\n",
        "\n",
        "rcp2 = recipe_vali2['RCP_PARTS_DTLS'].values.astype(str).tolist()\n",
        "\n",
        "max_len=1\n",
        "\n",
        "for idx,val in enumerate(rcp2):\n",
        "  one=val.split(',')\n",
        "  if len(one)>max_len:\n",
        "    max_len=len(one)\n",
        "  for idx2,val2 in enumerate(one):\n",
        "    for key,value in food_list.items():\n",
        "      if value in val2:\n",
        "\n",
        "        one[idx2]=key//1000\n",
        "      \n",
        "  rcp2[idx]=one\n",
        "\n",
        "input_ids=[]\n",
        "\n",
        "for idx,val in enumerate(rcp2):\n",
        "  input_ids_list=[]\n",
        "  for i in val:\n",
        "    if str(type(i))!=\"<class 'int'>\":\n",
        "      input_ids_list.append(0)\n",
        "    elif str(type(i))==\"<class 'int'>\":\n",
        "      input_ids_list.append(i)\n",
        "  input_ids.append(input_ids_list)\n",
        "\n",
        "for idx, val in enumerate(input_ids):\n",
        "  while len(val)!=max_len:\n",
        "    if len(val)==max_len:\n",
        "      break\n",
        "    else:\n",
        "      val.append(0)\n",
        "\n",
        "attention_masks=[]\n",
        "for idx,val in enumerate(input_ids):\n",
        "  attention_masks_list=[]\n",
        "  for i in val:\n",
        "    if i!=0:\n",
        "      attention_masks_list.append(1)\n",
        "    else:\n",
        "      attention_masks_list.append(0)\n",
        "  attention_masks.append(attention_masks_list)\n",
        "\n",
        "import torch\n",
        "input_ids = torch.LongTensor(input_ids)\n",
        "attention_masks = torch.LongTensor(attention_masks)\n",
        "\n",
        "vali_dataset = TensorDataset(input_ids, attention_masks)\n",
        "batch_size = 16\n",
        "\n",
        "vali_dataloader = DataLoader(\n",
        "    vali_dataset, sampler = SequentialSampler(vali_dataset), batch_size = batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzYgiABmHyr0"
      },
      "source": [
        "print('Predicting labels for {:,} Vegetarian Class...'.format(len(input_ids)))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for batch in vali_dataloader:\n",
        " \n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  b_input_ids, b_input_mask = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "      result = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  \n",
        "  logits=np.argmax(logits,axis=1)\n",
        "  for i in range(len(logits)):\n",
        "    predictions.append(logits[i])\n",
        "    \n",
        "\n",
        "print('    DONE.')\n",
        "\n",
        "recipe_vali2['Vegetarian_class'] = predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE3Y0uxsHyeu"
      },
      "source": [
        "# vegetarian class 정수 매핑 반환\n",
        "recipe_vali2['Vegetarian_class'] = recipe_vali2['Vegetarian_class'].replace([0,1,2,3,4,5],['채소', '달걀', '우유(유제품)', '생선,조개', '가금류', '소,돼지'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R4j7V0VIuci"
      },
      "source": [
        "# 예측 완료된 최종 df 제작\n",
        "recipe_final = pd.concat([recipe_test, recipe_vali2])\n",
        "\n",
        "recipe_final = recipe_final.reset_index()\n",
        "recipe_final = recipe_final.drop(['@id'], axis=1)\n",
        "recipe_final = recipe_final.reset_index()\n",
        "recipe_final = recipe_final.drop(['index'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSKhgjzqIuaV"
      },
      "source": [
        "# 예측 완료 df를 csv로 저장\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "output_file='recipe_final.csv'\n",
        "output_dir=Path('/content/drive/Shareddrives/2021 데이터청년캠퍼스/데이터베이스')\n",
        "\n",
        "output_dir.mkdir(parents=True,exist_ok=True)\n",
        "\n",
        "recipe_final.to_csv(output_dir/output_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JIgfn50IuX7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U1fHnDVIuVP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peLBNI0yIuSu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNtQTWquIuP9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUljkCyHIuMw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJPcIYrJIuJc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8s6nXKsIuDH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}